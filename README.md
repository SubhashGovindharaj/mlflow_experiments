# MLflow Experiments

This project demonstrates how to manage machine learning experiments using **MLflow**. It includes tracking of metrics, parameters, artifacts, and model versions in a reproducible workflow. The goal is to showcase MLOps best practices by integrating model experimentation with versioning and model registry.

---

## ğŸš€ Features

- End-to-end machine learning pipeline for regression/classification tasks
- Automatic logging of:
  - Parameters
  - Metrics
  - Artifacts (plots, data files)
  - Trained models
- Integration with **MLflow Tracking** and **Model Registry**
- Version control for reproducible experiments
- Modular pipeline: Data preprocessing, model training, and evaluation

---

## ğŸ“ Project Structure

mlflow_experiments/
â”‚
â”œâ”€â”€ data/                    # Sample data files
â”œâ”€â”€ notebooks/               # Jupyter notebooks for EDA and MLflow demo
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_loader.py       # Data loading & preprocessing
â”‚   â”œâ”€â”€ train.py             # ML model training and MLflow tracking
â”‚   â””â”€â”€ evaluate.py          # Model evaluation logic
â”œâ”€â”€ mlruns/                  # MLflow experiment logs (auto-generated)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“Š MLflow Tracking UI

Once you run the training script, launch MLflow UI to visualize experiments:

```bash
mlflow ui

By default, the UI is hosted at: http://localhost:5000

â¸»

ğŸ§ª How to Run

1. Install dependencies

pip install -r requirements.txt

python src/train.py

. View experiment results in MLflow UI

â¸»

âœ… Tech Stack
	â€¢	Python
	â€¢	Scikit-learn
	â€¢	Pandas, NumPy
	â€¢	Matplotlib / Seaborn
	â€¢	MLflow

â¸»

ğŸ“Œ Highlights
	â€¢	Demonstrates MLOps in action with minimal setup
	â€¢	Supports multiple model versions and comparison
	â€¢	Clean modular code for real-world extensibility

â¸»

ğŸ“¬ Contact

Created by Subhash Govindharaj
ğŸ“§ Email: subhashgovindharaj@gmail.com
